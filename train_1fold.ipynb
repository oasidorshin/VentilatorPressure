{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e4f1a-d750-494d-bb1a-d6bc4ce45ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from data import preprocessing, preprocessing, preprocessing2\n",
    "from modelling import LSTMModel_base, train_epoch, valid_epoch, L1Loss_masked\n",
    "from func import get_timestamp\n",
    "\n",
    "import timeit, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec073bd-f5bd-4b7f-aab7-287087949e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08837cc1-dde6-41e9-96c1-441cf19f348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kwargs = {'u_in_cumsum': True,\n",
    "          'u_in_lag12': True,\n",
    "          'u_in_lag34': False,\n",
    "          'u_in_lag_back12': True,\n",
    "          'u_in_lag_back34': False,\n",
    "          'u_in_diff12': True,\n",
    "          'u_in_diff34': False,\n",
    "          'u_in_diff_back12': True,\n",
    "          'u_in_diff_back34': False,\n",
    "          'u_in_last': False,\n",
    "          'u_in_max': False,\n",
    "          'scaler': RobustScaler()}\n",
    "train, test, features = preprocessing(train, test, **data_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379f00b-7177-4346-b392-466ed3866e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a7957-1a84-4032-9d54-3c9f92ae2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['pressure']\n",
    "train.drop(columns = ['id', 'breath_id', 'pressure'], inplace = True)\n",
    "test.drop(columns = ['id', 'breath_id'], inplace = True)\n",
    "input_size = train.shape[1]\n",
    "features, train.columns, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268ff2b-ff78-42c5-9580-4acec1c230dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "train = torch.tensor(train.to_numpy()).reshape(-1,80, input_size).float()\n",
    "test = torch.tensor(test.to_numpy()).reshape(-1,80, input_size).float()\n",
    "target = torch.tensor(target.to_numpy()).reshape(-1,80,1).float()\n",
    "\n",
    "train_dataset = TensorDataset(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637caa2-f427-4527-a886-0b64fe285b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    lr = 1e-03\n",
    "    \n",
    "    batch_size = 256\n",
    "    num_workers = 4\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    num_epochs = 500\n",
    "    k = 15\n",
    "    \n",
    "    lr_finder = False\n",
    "    training = True\n",
    "    \n",
    "cfg = Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b490ab-49bf-4734-a386-37ec5ffb940b",
   "metadata": {},
   "source": [
    "## LR finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769187e3-0739-4a53-b47d-2309de90396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.lr_finder:\n",
    "    model = LSTMModel_base(input_size)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, num_workers=cfg.num_workers)\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=cfg.device)\n",
    "    lr_finder.range_test(train_loader, start_lr = 0.00001, end_lr=1, num_iter=500)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,4)\n",
    "    lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "    lr_finder.reset() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa972d8-880d-4480-9642-63d5ce658251",
   "metadata": {},
   "source": [
    "## Train 1 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96abab6-d2d9-4e1c-b90b-40057cf3f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=cfg.k,shuffle=True,random_state=123)\n",
    "kf = list(kf.split(train))\n",
    "for i in range(cfg.k):\n",
    "    print(kf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1b0c2-8159-4257-9307-b2f755d1cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [2, 3]\n",
    "\n",
    "opt_criterion = L1Loss_masked()\n",
    "val_criterion = L1Loss_masked()\n",
    "\n",
    "device = cfg.device\n",
    "\n",
    "for fold in folds:\n",
    "    if cfg.training:    \n",
    "        train_idx,val_idx = kf[fold]\n",
    "        print(f\"Fold {fold}\", \"\\n\")\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, sampler=train_sampler, num_workers = cfg.num_workers)\n",
    "        val_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, sampler=val_sampler, num_workers = cfg.num_workers)\n",
    "\n",
    "        model = LSTMModel_base(input_size)\n",
    "        model.to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=0.01)\n",
    "        batch_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 50, T_mult = 1, eta_min = 1e-05)\n",
    "        #batch_scheduler = None\n",
    "        #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5)\n",
    "        scheduler = None\n",
    "        best_val_loss = 1000\n",
    "        best_weights = model.state_dict()\n",
    "        restart = 15\n",
    "\n",
    "        beg_time = timeit.default_timer()\n",
    "\n",
    "        for epoch in range(cfg.num_epochs):\n",
    "            start_time = timeit.default_timer()\n",
    "            train_loss = train_epoch(model,device,train_loader,opt_criterion,optimizer, epoch, batch_scheduler)\n",
    "            val_loss = valid_epoch(model,device,val_loader,val_criterion)\n",
    "            end_time = timeit.default_timer()\n",
    "\n",
    "            total = end_time - start_time\n",
    "\n",
    "            train_loss = np.mean(np.array(train_loss))\n",
    "            val_loss = np.mean(np.array(val_loss))\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_weights = copy.copy(model.state_dict())\n",
    "\n",
    "            print(f\"Epoch: {epoch} | T loss: {train_loss:.4f} V loss: {val_loss:.4f} Best: {best_val_loss:.4f} Time: {total:.4f}\")\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "            if epoch % 50 == 49:\n",
    "                print(f\"Total time passed: {((timeit.default_timer() - beg_time) / 60):.4f}\")\n",
    "\n",
    "        torch.save(best_weights, f\"models/new_local_fold_{fold}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7c1cc-2c43-40ab-8074-5d0e9b06791a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
